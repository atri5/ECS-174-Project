{
    "optimizer": "<class 'torch.optim.adam.Adam'>",
    "optimizer_kwargs": "{'lr': 0.0001}",
    "loss": "<class 'torch.nn.modules.loss.CrossEntropyLoss'>",
    "loss_kwargs": "{}",
    "nearly_stop": "5",
    "nepochs": "30",
    "batch_size": "32",
    "class_fn": "<class 'torch.nn.modules.activation.Softmax'>"
}