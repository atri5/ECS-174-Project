{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual seed: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 48692/48692 [00:04<00:00, 10075.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data: 4.84 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import logging\n",
    "\n",
    "\n",
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, image_dir, metadata_dir, transform=None, load_fraction=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with images organized by study_id, series_id.\n",
    "            metadata_dir (string): Directory containing the CSV files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            load_fraction (float, optional): Fraction of data to load for debugging (default 1).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.load_fraction = load_fraction\n",
    "        \n",
    "        # Load the coordinates and severity data from CSV files\n",
    "        self.coordinates = pd.read_csv(os.path.join(metadata_dir, 'train_label_coordinates.csv'))\n",
    "        self.metadata = pd.read_csv(os.path.join(metadata_dir, 'train.csv'))\n",
    "\n",
    "        # Define severity mapping for severity conditions\n",
    "        self.severity_mapping = {\n",
    "            'Normal/Mild': 0,\n",
    "            'Moderate': 1,\n",
    "            'Severe': 2\n",
    "        }\n",
    "\n",
    "        self.severity_columns = [\n",
    "            'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3', 'spinal_canal_stenosis_l3_l4', \n",
    "            'spinal_canal_stenosis_l4_l5', 'spinal_canal_stenosis_l5_s1',\n",
    "            'left_neural_foraminal_narrowing_l1_l2', 'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4', \n",
    "            'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1',\n",
    "            'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3', 'right_neural_foraminal_narrowing_l3_l4', \n",
    "            'right_neural_foraminal_narrowing_l4_l5', 'right_neural_foraminal_narrowing_l5_s1',\n",
    "            'left_subarticular_stenosis_l1_l2', 'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4', \n",
    "            'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1',\n",
    "            'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3', 'right_subarticular_stenosis_l3_l4', \n",
    "            'right_subarticular_stenosis_l4_l5', 'right_subarticular_stenosis_l5_s1'\n",
    "        ]\n",
    "\n",
    "        self.severity_levels = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "        \n",
    "        # Load only a fraction of the data for debugging\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Calculate the number of items to load based on the load_fraction\n",
    "        num_items = int(len(self.coordinates) * self.load_fraction)\n",
    "\n",
    "        for idx, row in tqdm(self.coordinates.iterrows(), total=num_items, desc=\"Loading images\"):\n",
    "            if len(data) >= num_items:\n",
    "                break\n",
    "\n",
    "            study_id = row['study_id']\n",
    "            series_id = row['series_id']\n",
    "            instance_number = row['instance_number']\n",
    "            condition_level = row['level']\n",
    "\n",
    "            severity = self.get_severity_for_level(study_id, condition_level)\n",
    "            \n",
    "            img_path = os.path.join(self.image_dir, f\"{study_id}/{series_id}/{instance_number}.dcm\")\n",
    "\n",
    "            coordinates = (row['x'], row['y'])\n",
    "            sample = {\n",
    "                'image_path': img_path,\n",
    "                'severity': severity,\n",
    "                'coordinates': coordinates\n",
    "            }\n",
    "            data.append(sample)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Time taken to load data: {end_time - start_time:.2f} seconds\")\n",
    "        return data\n",
    "\n",
    "    def get_severity_for_level(self, study_id, level):\n",
    "        severity_column = self.severity_columns[self.severity_levels.index(level)]\n",
    "        severity_row = self.metadata[self.metadata['study_id'] == study_id]\n",
    "        \n",
    "        if not severity_row.empty:\n",
    "            severity_value = severity_row[severity_column].values[0]\n",
    "            return self.severity_mapping.get(severity_value, -1)\n",
    "        return -1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img_path = sample[\"image_path\"]\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image = dicom_image.pixel_array\n",
    "        image = image.astype(np.float32) / np.max(image)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        sample['image'] = image\n",
    "        return sample\n",
    "\n",
    "# Set a manual seed for reproduction\n",
    "manual_seed = 110\n",
    "torch.manual_seed(manual_seed)\n",
    "print(f\"manual seed: {manual_seed}\")\n",
    "# Initialize the dataset\n",
    "image_dir = r\"Project\\train_images\"\n",
    "metadata_dir = r\"Project\"\n",
    "transform = transforms.Compose([ \n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "dataset = LumbarSpineDataset(image_dir=image_dir, metadata_dir=metadata_dir, transform=transform, load_fraction=1)\n",
    "\n",
    "# Create DataLoader with tqdm for progress bar\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Patch embedding for ViT\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=1, embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # Shape: (batch_size, embed_dim, num_patches_sqrt, num_patches_sqrt)\n",
    "        x = x.flatten(2)  # Shape: (batch_size, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "# Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)  # Shape: (batch_size, seq_len, 3 * embed_dim)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: (3, batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each has shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "# Feedforward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, dropout=0.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Transformer encoder layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.0):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))  # Multi-head attention\n",
    "        x = x + self.ffn(self.norm2(x))  # Feed-forward network\n",
    "        return x\n",
    "\n",
    "# Vision Transformer with dual output: classification and regression\n",
    "class VisionTransformerWithCoordinates(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=1,\n",
    "                 num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_dim=3072, dropout=0.0):\n",
    "        super(VisionTransformerWithCoordinates, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Classification token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        # Positional encoding\n",
    "        self.pos_embed = PositionalEncoding(num_patches, embed_dim)\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, dropout) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)  # Classification head\n",
    "        self.coord_head = nn.Linear(embed_dim, 2)  # Coordinates regression head (2 values for x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: (batch_size, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # Shape: (batch_size, num_patches + 1, embed_dim)\n",
    "\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]  # Extract the CLS token output for classification\n",
    "\n",
    "        # Predict severity (classification)\n",
    "        severity_logits = self.cls_head(cls_output)\n",
    "\n",
    "        # Predict coordinates (regression)\n",
    "        coords_output = self.coord_head(cls_output)  # Predict (x, y) coordinates\n",
    "\n",
    "        return severity_logits, coords_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Batch [620/1522] - Severity Loss: 0.2940, Coord Loss: 37516.0391, Total Loss: 37.8101\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Accumulate epoch losses for reporting\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m epoch_severity_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m severity_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     59\u001b[0m epoch_coord_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m coord_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     60\u001b[0m epoch_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming the model and dataloader are already defined as in your code:\n",
    "model = VisionTransformerWithCoordinates(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_dim=3072,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss functions\n",
    "severity_criterion = nn.CrossEntropyLoss()  # For severity classification\n",
    "coordinate_criterion = nn.MSELoss()  # For coordinate regression\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5  # Set the number of epochs\n",
    "print_frequency = 10  # Print loss after every 10 batches\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_severity_loss = 0.0\n",
    "    epoch_coord_loss = 0.0\n",
    "    epoch_total_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        images = batch['image'].to(device)\n",
    "        severity = batch['severity'].long().to(device)\n",
    "        \n",
    "        # Assuming batch['coordinates'] is a list of (x, y) for each sample\n",
    "        coords_list = [list(c) for c in batch['coordinates']]\n",
    "        coordinates = torch.tensor(coords_list, dtype=torch.float32).to(device)  # Shape (B, 2)\n",
    "        coordinates = coordinates.transpose(0, 1)\n",
    "\n",
    "        severity_logits, coord_preds = model(images)  # Ensure this is (B, 2)\n",
    "        \n",
    "        severity_loss = severity_criterion(severity_logits, severity)\n",
    "        coord_loss = coordinate_criterion(coord_preds, coordinates)\n",
    "        total_loss = severity_loss + 0.001 * coord_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Accumulate epoch losses for reporting\n",
    "        epoch_severity_loss += severity_loss.item()\n",
    "        epoch_coord_loss += coord_loss.item()\n",
    "        epoch_total_loss += total_loss.item()\n",
    "\n",
    "        # Print interim results\n",
    "        if (batch_idx + 1) % print_frequency == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                f\"Batch [{batch_idx+1}/{len(dataloader)}] \"\n",
    "                f\"- Severity Loss: {severity_loss.item():.4f}, \"\n",
    "                f\"Coord Loss: {coord_loss.item():.4f}, \"\n",
    "                f\"Total Loss: {total_loss.item():.4f}\",\n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "    # Print epoch results\n",
    "    avg_severity_loss = epoch_severity_loss / len(dataloader)\n",
    "    avg_coord_loss = epoch_coord_loss / len(dataloader)\n",
    "    avg_total_loss = epoch_total_loss / len(dataloader)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"- Avg Severity Loss: {avg_severity_loss:.4f}, \"\n",
    "        f\"Avg Coord Loss: {avg_coord_loss:.4f}, \"\n",
    "        f\"Avg Total Loss: {avg_total_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
